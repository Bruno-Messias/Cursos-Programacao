{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Computer Vision\nThis notebook includes some examples of techniques used to analyze images - which is a common requirement in AI solutions.\n\n## Manipulating Images\nAs far as computers are concerned, images are simply numerical data representations. You can use statistical techniques to manipulate and analyze the numerical properties of images.\n\n### Load an Image\nLet's start by loading a JPG file and examining its properties. Run the following cell to load and display an image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from matplotlib.pyplot import imshow\nfrom PIL import Image\nimport numpy as np\n%matplotlib inline\n\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg -o img.jpg\n\ni = np.array(Image.open('img.jpg'))\nimshow(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Examine Numerical Properties of the Image\nYou can clearly see that this is an image, but how does the computer interpret the data?\n\nRun the cell below to determine the data type of the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "type(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The image data is actually stored as a multi-dimensional array.\n\nLet's see what data type the array elements are:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "i.dtype",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "OK, so the array consists of 8-bit integer values. In other words, whole numbers between 0 and 255.\n\nNow let's examine the shape of the array:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "i.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So the image data is a three dimensional 433 x 650 x 3 array.\n\nThis is a RGB color JPG image sized 433 x 650 pixels. The image includes pixel values for red, green, and blue color channels. \n\nTo keep things simple, let's convert the image to a greyscale image so we only have one color channel dimension to deal with:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import skimage.color as sc\n\ni_mono = sc.rgb2gray(i)\nimshow(i_mono, cmap='gray')\ni_mono.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View Pixel Value Distributions\nLet's look at the distribution of pixel values in the image. Ideally, the image should have relatively even distribution of values, indicating good contrast and making it easier to extract analytical information.\n\nAn easy way to check this is to plot a histogram."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def im_hist(img):\n    import matplotlib.pyplot as plt    \n    fig = plt.figure(figsize=(8, 6))\n    fig.clf()\n    ax = fig.gca()    \n    ax.hist(img.flatten(), bins = 256)\n    plt.show()\n\nim_hist(i_mono)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another useful way to visualize the statistics of an image is as a cumulative distribution function (CDF) plot. Ideally, this should result in a fairly straight diagonal line."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def im_cdf(img):\n    import matplotlib.pyplot as plt    \n    fig = plt.figure(figsize=(8, 6))\n    fig.clf()\n    ax = fig.gca()    \n    ax.hist(img.flatten(), bins = 256, cumulative=True)\n    plt.show()\n    \nim_cdf(i_mono)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The histogram and CDF for our image show pretty uneven distribution. Ideally we should equalize the values in the image to improve its analytical value.\n\n### Equalize the Image\nHistogram equalization is often used to improve the statistics of images. In simple terms, the histogram equalization algorithm attempts to adjust the pixel values in the image to create a more uniform distribution. The code in the cell below uses the  **exposure.equalize_hist** method from the **skimage** package to equalize the image.  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from skimage import exposure\n\ni_eq = exposure.equalize_hist(i_mono)\nimshow(i_eq, cmap='gray')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's see what that's done to the histogram and CDF plots:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "im_hist(i_eq)\nim_cdf(i_eq)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The pixel intensities are more evenly distributed in the equalized image."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Denoising with Filters\n\nOften images need to be cleaned up to remove \"salt and pepper\" noise."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Add Some Random Noise\nLet's add some random noise to our image - such as you might see in a photograph taken in low light or at a low resolution."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import skimage\ni_n = skimage.util.random_noise(i_eq)\nimshow(i_n, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using a Gaussian Filter\nA Gaussian filter applies a weighted average (mean) value for pixels based on the pixels that surround them."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def gauss_filter(im, sigma = 2):\n    from scipy.ndimage.filters import gaussian_filter as gf\n    import numpy as np\n    return gf(im, sigma = sigma)   \ni_g = gauss_filter(i_n)\nimshow(i_g, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using a Median Filter\nThe Gaussian filter results in a blurred image - we could try a median filter, which as the name suggests applies the median value to pixels based on the pixels around them."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def med_filter(im, size = 2):\n    from scipy.ndimage.filters import median_filter as mf\n    import numpy as np\n    return mf(im, size = size)     \ni_m = med_filter(i_n)\nimshow(i_m, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Extract Features\nNow that we've done some initial processing of the image to improve its statistics for analysis, we can start to extract features from it.\n#### Sobel Edge Detection\nAs a first step in extracting features, you will apply the Sobel edge detection algorithm. This finds regions of the image with large gradient values in multiple directions. Regions with high omnidirectional gradient are likely to be edges or transitions in the pixel values. \n\nThe code in the cell below applies the Sobel algorithm to the median filtered image, using these steps:\n\n    1. Convert the color, rgb, image to grayscale (in this case, the image is already grayscale - but you should always do this because using a grayscale image simplifies the gradient calculation since it is two dimensional.\n    2. Computer the gradient in the x and y (horizontal and vertical) directions. \n    3. Compute the magnitude of the gradient.\n    4. Normalize the gradient values. \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def edge_sobel(image):\n    from scipy import ndimage\n    import skimage.color as sc\n    import numpy as np\n    image = sc.rgb2gray(image) # Convert color image to gray scale\n    dx = ndimage.sobel(image, 1)  # horizontal derivative\n    dy = ndimage.sobel(image, 0)  # vertical derivative\n    mag = np.hypot(dx, dy)  # magnitude\n    mag *= 255.0 / np.amax(mag)  # normalize (Q&D)\n    mag = mag.astype(np.uint8)\n    return mag\n\ni_edge = edge_sobel(i_m)\nimshow(i_edge, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's try with the more blurred gaussian filtered image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "i_edge = edge_sobel(i_g)\nimshow(i_edge, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that the lines are more pronounced. Although a gaussian filter makes the image blurred to human eyes, this blurring can actually help accentuate contrasting features."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Harris Corner Detection\nAnother example of a feature extraction algorithm is corner detection. In simple terms, the Harris corner detection algorithm locates regions of the image with large changes in pixel values in all directions. These regions are said to be corners. The Harris corner detector is paired with the **corner_peaks** method. This operator filters the output of the Harris algorithm, over a patch of the image defined by the span of the filters, for the most likely corners."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to apply the Harris corner-detection algorithm to an image\ndef corner_harr(im, min_distance = 20):\n    from skimage.feature import corner_harris, corner_peaks\n    mag = corner_harris(im)\n    return corner_peaks(mag, min_distance = min_distance)\n\n# Find the corners in the median filtered image with a minimum distance of 20 pixels\nharris = corner_harr(i_m, 20)\n\nprint (harris)\n\n# Function to plot the image with the harris corners marked on it\ndef plot_harris(im, harris, markersize = 20, color = 'red'):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig = plt.figure(figsize=(6, 6))\n    fig.clf()\n    ax = fig.gca()    \n    ax.imshow(np.array(im).astype(float), cmap=\"gray\")\n    ax.plot(harris[:, 1], harris[:, 0], 'r+', color = color, markersize=markersize)\n    return 'Done'  \n\nplot_harris(i_m, harris)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The corner detection algorithm has identified the eyes in the image."
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Using the Computer Vision API\nThe techniques used so far in this notebook show how you can perform simple image amnipulation and apply some popular algorithms to analyze images. More complex image analysis capabilities are encapsulated in the Computer Vision API cognitive service.\n\n### Create a Computer Vision API Service\nTo provision a Computer Vision API service in your Azure subscription, Follow these steps:\n\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n4. In the list of cognitive services, click **Computer Vision API**.\n5. In the **Computer Vision API** blade, click **Create**.\n6. In the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Your Azure subscription.\n  * **Location**: Choose the Azure datacenter location where you want to host your service.\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you created in the previous lab (or create a new one if you didn't complete the previous lab)\n  * Read the notice about the use of your data, and select the checkbox.\n7. Wait for the service to be created.\n8. When deployment is complete, click **All Resources** and then click your Computer Vision service to open its blade.\n9. In the blade for your Computer Vision service, note the **Endpoint** URL. Then assign the base URI (*location*.api.cognitive.microsoft.com) for your service to the **visionURI** variable in the cell below.\n10. In the blade for your Computer Vision service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **visionKey** variable assignment value in the cell below. \n11. Run the cell below to assign the variables.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "visionURI = 'LOCATION.api.cognitive.microsoft.com'\nvisionKey = 'YOUR_KEY'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get An Image from a URL\nLet's start with the same image we analyzed previously.\n\nRun the code in the cell below to retrieve the original color image from its URL:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nimg_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg'\n\n# Get the image and show it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nimshow(img)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use the Computer Vision API to Get Image Features\nThe Computer Vision API uses a machine learning model that has been trtained with millions of images. It can extract features from images and return a suggested description, as well as details about the image file and a suggested list of \"tags\" that apply to it.\n\nRun the cell below to see what caption the Computer Vision API suggests for the image above."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_image_features(img_url):\n    import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n\n    headers = {\n        # Request headers.\n        'Content-Type': 'application/json',\n        'Ocp-Apim-Subscription-Key': visionKey,\n    }\n\n    params = urllib.parse.urlencode({\n        # Request parameters. All of them are optional.\n        'visualFeatures': 'Categories,Description,Color',\n        'language': 'en',\n    })\n\n    body = \"{'url':'\" + img_url + \"'}\"\n\n    try:\n        # Execute the REST API call and get the response.\n        conn = http.client.HTTPSConnection(visionURI)\n        conn.request(\"POST\", \"/vision/v1.0/analyze?%s\" % params, body, headers)\n        response = conn.getresponse()\n        data = response.read()\n\n        # 'data' contains the JSON response.\n        parsed = json.loads(data.decode(\"UTF-8\"))\n        if response is not None:\n            return parsed\n        conn.close()\n\n\n    except Exception as e:\n        print('Error:')\n        print(e)\n        \njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The description is reasonably, if not exactly, appropriate.\n\nRun the cell below to see the full JSON response, including image properties and suggested tags."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\n\n# View the full details returned\nprint (json.dumps(jsonData, sort_keys=True, indent=2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try with a different image:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/uke.jpg'\n\n# Get the image and show it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nimshow(img)\njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How about something a little more complex?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/soccer.jpg'\n\n# Get the image and show it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nimshow(img)\njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Using the Face API\nWhile the Computer Vision API is useful for general image analysis, the Face API offers specific functions for analyzing faces in images. This can be useful in a variety of AI scenarios.\n\n### Create a Face API Service\nTo provision a Computer Vision API service in your Azure subscription, Follow these steps:\n\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n4. In the list of cognitive services, click **Face**.\n5. In the **Face** blade, click **Create**.\n6. In the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Your Azure subscription.\n  * **Location**: Choose the Azure datacenter location where you want to host your service.\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you created in the previous lab (or create a new one if you didn't complete the previous lab)\n  * Read the notice about the use of your data, and select the checkbox.\n7. Wait for the service to be created.\n8. When deployment is complete, click **All Resources** and then click your Face service to open its blade.\n9. In the blade for your Face service, copy the *full* **Endpoint** URL (including the *https* prefix and */face/v1.0/* path), and paste it into the **faceURI** variable assignment value in the cell below.\n10. In the blade for your Face service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **faceKey** variable assignment value in the cell below. \n11. Run the cell below to assign the variables."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "faceURI = \"https://LOCATION.api.cognitive.microsoft.com/face/v1.0/\"\nfaceKey = \"YOUR_KEY\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Face API has a Python SDK, which you can install as a package. This makes it easier to work with.\n\nRun the following cell to install the Face SDK."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install cognitive_face",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you're ready to use the Face API. First, let's see if we can detect a face in an image:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import cognitive_face as CF\n\n# Set URI and Key\nCF.BaseUrl.set(faceURI)\nCF.Key.set(faceKey)\n\n\n# Detect faces in an image\nimg_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme1.jpg'\nresult = CF.face.detect(img_url)\nprint (result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Face API has detected one face, and assigned it an ID. It also returns the coordinates for the top left corner and the width and height for the rectangle within which the face is detected.\n\nRun the cell below to show the rectange on the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport requests\nfrom io import BytesIO\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image, ImageDraw\n\n# Get the image\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\n\n# Add rectangles for each face found\ncolor=\"blue\"\nif result is not None:\n    draw = ImageDraw.Draw(img) \n    for currFace in result:\n        faceRectangle = currFace['faceRectangle']\n        left = faceRectangle['left']\n        top = faceRectangle['top']\n        width = faceRectangle['width']\n        height = faceRectangle['height']\n        draw.line([(left,top),(left+width,top)],fill=color, width=5)\n        draw.line([(left+width,top),(left+width,top+height)],fill=color , width=5)\n        draw.line([(left+width,top+height),(left, top+height)],fill=color , width=5)\n        draw.line([(left,top+height),(left, top)],fill=color , width=5)\n\n# show the image\nimshow(img)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As well as detecting the face, the Face API assigned an ID to this face. The ID is retained by the service for a while, enabling you to reference it. Run the following cell to see the ID assigned to the face that has been detected:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "face1 = result[0]['faceId']\nprint (\"Face 1:\" + face1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One useful thing you can do with the face ID is, is to use it to compare another image and see if a matching face is found. This kind of facial comparison is common in a variety of security / user authentication scenarios.\n\nLet's try it with another image of the same person:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nif result2 is not None:\n    face2 = result2[0]['faceId']\n    print (\"Face 2:\" + face2)\n\ndef verify_face(face1, face2):\n    # By default, assume the match is unverified\n    verified = \"Not Verified\"\n    color=\"red\"\n\n    # compare the comparison face to the original one we retrieved previously\n    verify = CF.face.verify(face1, face2)\n\n    # if there's a match, set verified and change color to green\n    if verify['isIdentical'] == True:\n        verified = \"Verified\"\n        color=\"lightgreen\"\n\n    # Display the second face with a red rectange if unverified, or green if verified\n    draw = ImageDraw.Draw(img2) \n    for currFace in result2:\n        faceRectangle = currFace['faceRectangle']\n        left = faceRectangle['left']\n        top = faceRectangle['top']\n        width = faceRectangle['width']\n        height = faceRectangle['height']\n        draw.line([(left,top),(left+width,top)] , fill=color, width=5)\n        draw.line([(left+width,top),(left+width,top+height)] , fill=color, width=5)\n        draw.line([(left+width,top+height),(left, top+height)] , fill=color, width=5)\n        draw.line([(left,top+height),(left, top)] , fill=color, width=5)\n\n    # show the image\n    imshow(img2)\n\n    # Display verification status and confidence level\n    print(verified)\n    print (\"Confidence Level: \" + str(verify['confidence']))\n\nverify_face(face1, face2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service has matched the face in a similar photo, with a reasonably high confidence level.\n\nBut what about the same face in a different photo - maybe with a stylish goatee beard and sunglasses?:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme3.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nface2 = result2[0]['faceId']\nprint (\"Face 2:\" + face2)\n\nverify_face(face1, face2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, the face is matched - but with lower confidence reflecting the differences in the image.\n\nWhat if we try to match the original face to a different person?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/satya.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nface2 = result2[0]['faceId']\nprint (\"Face 2:\" + face2)\n\nverify_face(face1, face2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "No match!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Using the Custom Vision Cognitive Servive\nThe *Custom Vision* cognitive service enables you to create custom computer vision solutions.\n\nIn this notebook, you will create and train a Custom Vision *image classification* project that can identify pictures of apples and carrots, and use it to classify new images.\n\n> **Note**: *Some of the images used in the lab are sourced from the free image library at <a href='http://www.pachd.com' target='_blank'>www.pachd.com</a>*\n\n### Install the Custom Vision SDK\nThe first step is to install the Python SDK for the Custom Vision service:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install the Custom Vision SDK\n! pip install azure-cognitiveservices-vision-customvision",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now lets download and extract the images you will use to train your classifier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/produce.zip -o produce.zip\n!unzip produce.zip",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a Custom Vision service instance\nNow you're ready to use the Custom Vision service. You'll need to create an instance of the service and get your unique training and prediction keys so you can access it:\n1. If you don't already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com/Account/Free.\n2. Go to https://customvision.ai/ and sign in using the Microsoft account associated with your Azure subscription.\n3. Create a custom vision service in your subscription. Specify any available location, and select the **S0** pricing tier.\n4. Click the *Settings* (&#9881;) icon at the top right to view your *training key* and *prediction key*. Then assign these to the variables below, change the location in the *endpoint* as necessary, and run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "TRAINING_KEY = 'YOUR_TRAINING_KEY'\nPREDICTION_KEY = 'YOUR_PREDICTION_KEY'\nENDPOINT='https://YOUR_REGION.api.cognitive.microsoft.com' # Use just the base URL - https://<region>.api.cognitive.microsoft.com\nPREDICTION_RESOURCE_ID=\"/subscriptions/YOUR_SUBSCRIPTION_ID/resourceGroups/YOUR_RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/YOUR_ACCOUNT_Prediction\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a Custom Vision project\nNow we'll create a project for the apple/carrot classifier:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n\ntrainer = CustomVisionTrainingClient(TRAINING_KEY, endpoint=ENDPOINT)\n\n# Create a new project\nprint (\"Creating project...\")\nproject = trainer.create_project(\"Produce Classification\")\nprint(\"Created project!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Add tags\nThe project will identify images as apples or carrots, so we'll need tags for those classes:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make two tags in the new project\nprint(\"Creating tags...\")\napple_tag = trainer.create_tag(project.id, \"Apple\")\ncarrot_tag = trainer.create_tag(project.id, \"Carrot\")\nprint('Created tags!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Upload training images\nNow that we've got the tags, we need to upload some images of apples and carrots, assign the appropriate tags:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nprint(\"Adding images...\")\n\napples_dir = \"apples\"\nfor image in os.listdir(apples_dir):\n    with open(os.path.join(apples_dir, image), mode=\"rb\") as img_data: \n        trainer.create_images_from_data(project.id, img_data.read(), [apple_tag.id])\n\ncarrots_dir = \"carrots\"\nfor image in os.listdir(carrots_dir):\n    with open(os.path.join(carrots_dir, image), mode=\"rb\") as img_data: \n        trainer.create_images_from_data(project.id, img_data.read(), [carrot_tag.id])\n        \nprint('Added images!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Return to your Custom Vision service and click the *Home* (&#8962;) icon to return to the home page, and then open the ***Apple or Carrot*** project to view the images that have been uploaded and tagged.\n\n### Train the project\nWith the tagged images in place, we're now ready to train a classification model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\nprint (\"Training...\")\n# Train the project, checking status every 1 second\niteration = trainer.train_project(project.id)\nwhile (iteration.status == \"Training\"):\n    iteration = trainer.get_iteration(project.id, iteration.id)\n    print (\"Training status: \" + iteration.status)\n    time.sleep(1)\n\n# The iteration is now trained. Publish it to the project endpoint\ntrainer.publish_iteration(project.id, iteration.id, \"First Iteration\", PREDICTION_RESOURCE_ID)\n\n# Make it the default iteration\niteration = trainer.update_iteration(project_id= project.id, iteration_id=iteration.id, name= \"First Iteration\", is_default=True)\n\nprint (\"Trained!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Return to your Custom Vision service and click the **Performance** tab to view the *Precision* and *Recall* metrics for your trained project. These should be pretty high, even through we only used a few images.\n\n### Use the project to classify images\nNow that we have a trained project, we can use it to predict the class of new images that weren't in the training dataset:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n%matplotlib inline\n\n# Use two test images\ntest_img1_url = 'http://www.pachd.com/free-images/food-images/apple-01.jpg'\ntest_img2_url = 'http://www.pachd.com/free-images/food-images/carrot-02.jpg'\n\ntest_image_urls = []\ntest_image_urls.append(test_img1_url)\ntest_image_urls.append(test_img2_url)\n\n# Create an instance of the prediction service\npredictor = CustomVisionPredictionClient(PREDICTION_KEY, endpoint=ENDPOINT)\n\n# Create a figure\nfig = plt.figure(figsize=(16, 8))\n\n# Get the images and show the predicted classes\nfor url_idx in range(len(test_image_urls)):\n    response = requests.get(test_image_urls[url_idx])\n    image_contents = Image.open(BytesIO(response.content))\n    results = predictor.classify_image_url(project_id=project.id, published_name=iteration.name, url=test_image_urls[url_idx])\n    # The results include a prediction for each tag, in descending order of probability - so we'll get the first one\n    prediction = results.predictions[0].tag_name + \": {0:.2f}%\".format(results.predictions[0].probability * 100)\n    # Subplot for image and its predicted class\n    a=fig.add_subplot(1,2,url_idx+1)\n    imgplot = plt.imshow(image_contents)\n    a.set_title(prediction)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}